\chapter{Referencial Teórico}\label{cap:referencial}


% PARTE DA VIC

\section{Computação: fundamentos e evolução histórica}

\subsection{Definição geral de computação}
De forma ampla, a computação pode ser definida como o conjunto de processos pelos quais um sistema recebe entradas, realiza algum tipo de processamento sobre essas informações e produz saídas \cite{kaiser2021learn}. Essa definição independe da natureza física do sistema computacional, sendo válida tanto para dispositivos mecânicos quanto eletrônicos ou digitais. O processamento pode envolver operações aritméticas, lógicas, simbólicas ou algorítmicas, desde que exista uma transformação bem definida entre os dados de entrada e os resultados obtidos na saída.

Sob essa perspectiva, a computação não se restringe aos computadores modernos, mas abrange qualquer mecanismo capaz de executar regras sistemáticas para manipulação de informação. Essa visão permite compreender a computação como um conceito anterior e mais amplo do que os próprios computadores digitais.


\subsection{Computação antes dos computadores digitais}
Muito antes do surgimento dos computadores eletrônicos, diferentes ferramentas e métodos foram desenvolvidos para auxiliar o ser humano em tarefas computacionais \cite{campbell2023computer}. Um dos exemplos mais antigos é o ábaco, utilizado por diversas civilizações como instrumento para realizar operações aritméticas básicas. Embora simples, o ábaco já incorpora a ideia fundamental de representação e manipulação sistemática de informações.

Durante os séculos seguintes, surgiram dispositivos mecânicos mais sofisticados, como as máquinas de cálculo de Blaise Pascal e Gottfried Wilhelm Leibniz, capazes de executar automaticamente somas, subtrações e, em alguns casos, multiplicações e divisões. Paralelamente, grande parte dos cálculos científicos e de engenharia continuava sendo realizada manualmente, frequentemente com o auxílio de tabelas matemáticas e métodos numéricos.

Esses exemplos evidenciam que a computação, entendida como processamento de informação, antecede a computação digital moderna, sendo impulsionada por necessidades práticas nas áreas de comércio, ciência, engenharia e navegação.


\subsection{Surgimento dos computadores digitais}
O avanço tecnológico do século XX possibilitou a transição da computação mecânica para a computação eletromecânica e, posteriormente, eletrônica. Os primeiros computadores eletromecânicos utilizavam relés para implementar operações lógicas e aritméticas, enquanto os computadores eletrônicos passaram a empregar válvulas a vácuo, permitindo velocidades de processamento significativamente maiores.

Um exemplo histórico emblemático é o uso de máquinas computacionais no contexto da Segunda Guerra Mundial, especialmente nos esforços de criptoanálise. A máquina Enigma, utilizada para cifrar comunicações militares, representava um problema computacional de grande complexidade para a época \cite{hodges2014alan}. O desenvolvimento de métodos sistemáticos e máquinas especializadas para quebrar suas cifras ilustra de forma clara como a computação passou a desempenhar um papel estratégico na resolução de problemas reais de alta relevância prática.

Esse período marca o início da computação digital como uma ferramenta central para lidar com problemas que extrapolavam a capacidade do cálculo manual ou puramente mecânico.

Após o surgimento dos primeiros computadores eletrônicos, a computação clássica passou por uma evolução acelerada, impulsionada principalmente por avanços no hardware. As válvulas a vácuo foram gradualmente substituídas por transistores, que ofereciam maior confiabilidade, menor consumo de energia e maior densidade de integração. Posteriormente, o desenvolvimento dos circuitos integrados permitiu a miniaturização e a integração de um número crescente de componentes em um único chip.

Esse avanço tecnológico é frequentemente associado à chamada Lei de Moore \cite{moore1965cramming}, que observa o crescimento aproximadamente exponencial do número de transistores em circuitos integrados ao longo do tempo. Como consequência, houve um aumento contínuo no poder computacional disponível, tanto em termos de velocidade quanto de capacidade de armazenamento.

Além do aumento da densidade de componentes, a computação clássica também evoluiu por meio do paralelismo, explorando arquiteturas com múltiplos núcleos, processamento vetorial e sistemas distribuídos. Essas estratégias permitiram ganhos adicionais de desempenho, especialmente para aplicações que podem ser decompostas em tarefas paralelas.

\subsection{Limitações da computação clássica}
Apesar dos avanços expressivos, a computação clássica enfrenta limitações fundamentais quando aplicada a determinados tipos de problemas. Em muitos casos, o número de estados possíveis cresce exponencialmente com o tamanho do problema, tornando inviável a exploração exaustiva de todas as soluções em tempo razoável.

A teoria da complexidade computacional formaliza essas limitações por meio de classes como P, NP, NP-difícil e NP-completo \cite{sipser1996introduction}. Problemas pertencentes à classe P admitem soluções eficientes em tempo polinomial, enquanto problemas NP-completos, embora fáceis de verificar, não possuem algoritmos conhecidos que os resolvam eficientemente em computadores clássicos.

Exemplos práticos incluem problemas de otimização combinatória, como o problema do caixeiro viajante, a fatoração de grandes números inteiros e certos problemas de simulação de sistemas físicos complexos. Na prática, mesmo com hardware moderno e técnicas avançadas de paralelismo, esses problemas rapidamente se tornam intratáveis à medida que sua dimensão aumenta.

As limitações da computação clássica motivam a busca por novos paradigmas computacionais capazes de lidar de forma mais eficiente com problemas de alta complexidade. Em particular, há classes de problemas cuja estrutura sugere que abordagens baseadas exclusivamente em bits clássicos e operações determinísticas podem não ser suficientes.

Nesse contexto, surgem modelos alternativos de computação, como a computação quântica, que exploram princípios físicos distintos para representar e processar informações. Esses modelos não substituem a computação clássica, mas oferecem novas ferramentas conceituais e práticas para enfrentar problemas que “pedem” um novo tipo de tratamento computacional, abrindo caminho para avanços em áreas como criptografia, otimização e simulação de sistemas complexos.


\section{Limites da computação clássica e motivação para novos paradigmas}

\subsection{Limitações tecnológicas versus limitações teóricas}
Ao longo da história da computação, muitos obstáculos foram superados por meio de avanços tecnológicos, como o aumento da velocidade dos processadores, a miniaturização dos componentes eletrônicos e o aprimoramento das arquiteturas computacionais. Essas limitações, de natureza tecnológica, estão associadas a restrições práticas de engenharia e tendem a ser mitigadas com o progresso científico e industrial.

Porém, nem todas as limitações enfrentadas pela computação clássica podem ser superadas por avanços tecnológicos, existem barreiras de natureza teórica, intrínsecas ao próprio modelo computacional clássico, baseado na manipulação de bits e em operações determinísticas ou probabilísticas.

Essas limitações persistem mesmo quando se considera um computador idealizado, capaz de executar operações em tempo arbitrariamente pequeno e com memória ilimitada. Nesses casos, o fator limitante não é a velocidade de cada operação individual, mas o crescimento do número total de operações necessárias para resolver determinados problemas.

Um exemplo paradigmático é o problema do logaritmo discreto \cite{shor1994algorithms}. Dado um grupo finito, um gerador \( g \) e um elemento \( h \), o objetivo é encontrar o expoente \( x \) tal que \( g^x = h \). Apesar de sua formulação simples, não existem algoritmos clássicos conhecidos que resolvam esse problema em tempo polinomial, mesmo considerando paralelismo clássico extremo. O melhor desempenho clássico conhecido ainda apresenta crescimento subexponencial, tornando o problema intratável para instâncias de grande porte. Assim, a dificuldade do logaritmo discreto ilustra um limite estrutural do modelo clássico de computação, e não apenas uma limitação tecnológica.

\subsection{Crescimento exponencial do espaço de estados e exemplos computacionais}
O crescimento exponencial do espaço de estados está no cerne de diversos problemas computacionais relevantes. Na teoria da complexidade, esse fenômeno aparece de forma clara em problemas pertencentes às classes NP-difícil e NP-completo \cite{sipser1996introduction}, nos quais o número de soluções candidatas cresce combinatorialmente com o tamanho do problema.

O problema do caixeiro viajante (\textit{Traveling Salesman Problem} – TSP) é um exemplo clássico. Dado um conjunto de cidades e as distâncias entre elas, o objetivo é encontrar o caminho de menor custo que visite cada cidade exatamente uma vez. A versão de decisão do problema é NP-completa, e a versão de otimização é NP-difícil. Na prática, instâncias de grande porte são tratadas por meio de heurísticas e algoritmos aproximados, como algoritmos genéticos ou \textit{simulated annealing} \cite{kirkpatrick1983optimization}, que produzem boas soluções, mas sem garantia de otimalidade global.

Outro exemplo de grande relevância é o problema da fatoração de números inteiros grandes, base de sistemas criptográficos como o RSA \cite{rivest1978method}. Embora não se saiba se esse problema é NP-completo, ele não admite algoritmos clássicos eficientes conhecidos. Nesse caso, a computação quântica oferece uma vantagem estrutural: o algoritmo de Shor \cite{shor1994algorithms} demonstra que a fatoração pode ser realizada em tempo polinomial em um computador quântico, evidenciando que o obstáculo não está apenas no hardware, mas no modelo computacional utilizado.

Essas dificuldades tornam-se ainda mais pronunciadas na simulação de sistemas físicos quânticos. Um sistema composto por $n$ partículas quânticas requer a descrição de um estado em um espaço de dimensão proporcional a $2^n$. Como consequência, a simulação exata desses sistemas em computadores clássicos torna-se rapidamente inviável, mesmo para valores modestos de $n$, reforçando a ideia de que certos problemas apresentam uma incompatibilidade fundamental com o paradigma clássico de computação.

\subsection{Computação não-clássica e a motivação física de Feynman}
Diante dessas limitações estruturais, surgem propostas de paradigmas de computação não-clássica, que buscam explorar modelos físicos alternativos para representar e processar informação. A computação analógica utiliza grandezas contínuas e a dinâmica natural de sistemas físicos para realizar cálculos, enquanto a computação neuromórfica inspira-se na organização do cérebro, explorando paralelismo massivo e adaptação.

Entre esses paradigmas, a computação quântica se destaca por sua fundamentação direta nas leis da mecânica quântica. Em vez de tentar simular sistemas quânticos por meio de bits clássicos, a computação quântica propõe utilizar sistemas quânticos controlados como elementos computacionais, explorando propriedades como superposição, interferência e emaranhamento.

Essa ideia foi articulada de forma pioneira por Richard Feynman \cite{feynman2018simulating}, ao observar que a dificuldade de simular sistemas quânticos em computadores clássicos não é acidental, mas resulta do fato de que a natureza não opera segundo as regras da computação clássica. Assim, a computação quântica surge não apenas como uma alternativa tecnológica, mas como um novo paradigma conceitual, alinhado à própria estrutura física dos sistemas que se deseja estudar.

%%% 4.

\section{Fundamentos de Mecânica Quântica para Computação}

Este capítulo apresenta os conceitos essenciais da mecânica quântica necessários para a compreensão da computação quântica. O objetivo não é desenvolver a teoria completa da mecânica quântica, mas introduzir apenas os elementos matemáticos e conceituais que fundamentam o modelo computacional quântico, conforme apresentado em referências clássicas como \cite{nielsen2010quantum} e \cite{watrous2018theory}.

\subsection{Espaços Vetoriais Complexos}

A formulação matemática da mecânica quântica é baseada em espaços vetoriais complexos. Em computação quântica, os estados físicos de sistemas quânticos são representados por vetores em um espaço vetorial complexo de dimensão finita, usualmente um espaço de Hilbert.

Um espaço vetorial complexo é definido sobre o corpo dos números complexos $\mathbb{C}$, permitindo combinações lineares do tipo
\begin{equation}
\alpha \ket{v} + \beta \ket{w}, \quad \alpha, \beta \in \mathbb{C}.
\end{equation}

Além disso, esse espaço é equipado com um produto interno, que permite definir normas, ângulos e projeções, elementos fundamentais para a interpretação probabilística da teoria quântica.

\subsection{Estados Quânticos e Notação de Dirac}

Estados quânticos puros são representados por vetores normalizados em um espaço de Hilbert complexo. A notação mais comum para representar esses vetores é a notação de Dirac, ou notação bra-ket.

Um vetor de estado é denotado por um \emph{ket}, como $\ket{\psi}$, enquanto seu conjugado transposto é denotado por um \emph{bra}, $\bra{\psi}$. O produto interno entre dois estados é escrito como
\begin{equation}
\braket{\phi|\psi}.
\end{equation}

No contexto da computação quântica, o sistema mais básico é o \emph{qubit}, cujo estado geral pode ser escrito como
\begin{equation}
\ket{\psi} = \alpha \ket{0} + \beta \ket{1},
\end{equation}
onde $\alpha, \beta \in \mathbb{C}$ e satisfazem a condição de normalização
\begin{equation}
|\alpha|^2 + |\beta|^2 = 1.
\end{equation}

\subsection{Superposição}

A superposição é uma das características centrais da mecânica quântica e não possui análogo direto na computação clássica. Enquanto um bit clássico assume apenas os valores 0 ou 1, um qubit pode estar em uma combinação linear desses dois estados.

Essa propriedade permite que sistemas quânticos representem simultaneamente múltiplos estados clássicos, o que é fundamental para o paralelismo quântico explorado por algoritmos quânticos.

É importante ressaltar que a superposição não implica que o sistema possui simultaneamente ambos os valores clássicos de forma observável, mas sim que seu estado físico é descrito por uma combinação coerente desses estados.

\subsection{Probabilidade e Amplitudes}

Diferentemente da computação clássica, na qual probabilidades são atribuídas diretamente aos estados, na mecânica quântica as probabilidades são obtidas a partir das amplitudes complexas associadas aos estados.

Dado um estado
\begin{equation}
\ket{\psi} = \alpha \ket{0} + \beta \ket{1},
\end{equation}
a probabilidade de obter o resultado $\ket{0}$ em uma medição é $|\alpha|^2$, enquanto a probabilidade de obter $\ket{1}$ é $|\beta|^2$.

As amplitudes complexas permitem fenômenos exclusivamente quânticos, como interferência construtiva e destrutiva, que desempenham papel central em algoritmos quânticos como o de Shor e o de Grover.

\subsection{Medição e Colapso do Estado}

A medição em mecânica quântica é um processo fundamentalmente probabilístico. Ao medir um sistema quântico, o estado do sistema colapsa para um dos estados da base associada ao observável medido.

No caso de um qubit medido na base computacional $\{\ket{0}, \ket{1}\}$, o estado $\ket{\psi} = \alpha \ket{0} + \beta \ket{1}$ colapsa para $\ket{0}$ com probabilidade $|\alpha|^2$ ou para $\ket{1}$ com probabilidade $|\beta|^2$.

Após a medição, a superposição é destruída, e medições subsequentes produzem o mesmo resultado com probabilidade unitária. Esse comportamento distingue claramente a evolução unitária dos sistemas quânticos do processo de medição.

\subsection{Sistemas Compostos e Produto Tensorial}

Para descrever sistemas quânticos compostos por múltiplos subsistemas, utiliza-se o produto tensorial de espaços vetoriais. Se dois sistemas possuem espaços de estados $\mathcal{H}_A$ e $\mathcal{H}_B$, o espaço de estados do sistema composto é dado por
\begin{equation}
\mathcal{H}_{AB} = \mathcal{H}_A \otimes \mathcal{H}_B.
\end{equation}

No caso de dois qubits, os estados base do sistema composto são
\begin{equation}
\{ \ket{00}, \ket{01}, \ket{10}, \ket{11} \}.
\end{equation}

O produto tensorial é essencial para representar correlações quânticas entre subsistemas e constitui a base matemática para o estudo do emaranhamento.

\subsection{Emaranhamento Quântico}

O emaranhamento é um fenômeno exclusivamente quântico no qual o estado de um sistema composto não pode ser escrito como o produto tensorial dos estados de seus subsistemas.

Um exemplo clássico de estado emaranhado é o estado de Bell
\begin{equation}
\ket{\Phi^+} = \frac{1}{\sqrt{2}}(\ket{00} + \ket{11}).
\end{equation}

Nesse estado, nenhuma descrição individual dos subsistemas é suficiente para caracterizar o sistema completo. Medições realizadas em um subsistema afetam instantaneamente as probabilidades associadas ao outro, independentemente da distância entre eles.

O emaranhamento é um recurso fundamental para a computação quântica, sendo explorado em protocolos como teleportação quântica, correção de erros quânticos e aceleração algorítmica.

\subsection{Informação Clássica versus Informação Quântica}

A diferença conceitual entre informação clássica e quântica vai além da representação dos dados. Na computação clássica, a informação é armazenada em bits que assumem valores bem definidos e podem ser copiados livremente.

Na computação quântica, a informação é armazenada em estados quânticos, que não podem ser copiados arbitrariamente devido ao teorema da não clonagem. Além disso, a informação quântica está sujeita a restrições impostas pela medição e pelo colapso do estado.

Essas diferenças impõem novos paradigmas computacionais, nos quais algoritmos devem ser cuidadosamente projetados para explorar superposição, interferência e emaranhamento sem destruir a informação relevante durante o processo computacional.


% 5.

\section{Qubits e Informação Quântica}

Esta seção tem como objetivo formalizar o conceito de qubit e introduzir os fundamentos da informação quântica. Embora conceitos como superposição e medição já tenham sido apresentados anteriormente, aqui eles são tratados de forma mais sistemática, estabelecendo a base conceitual necessária para o estudo de circuitos quânticos, algoritmos e canais quânticos.

\subsection{Definição Formal de Qubit}

O bit quântico (\textit{quantum bit}), ou qubit, é a unidade fundamental de informação na computação quântica. Matematicamente, um qubit é definido como um vetor unitário pertencente a um espaço de Hilbert complexo bidimensional, usualmente denotado por $\mathbb{C}^2$.

Escolhendo a base computacional $\{\ket{0}, \ket{1}\}$, qualquer estado puro de um qubit pode ser escrito como
\begin{equation}
\ket{\psi} = \alpha \ket{0} + \beta \ket{1},
\end{equation}
onde $\alpha, \beta \in \mathbb{C}$ e satisfazem a condição de normalização
\begin{equation}
|\alpha|^2 + |\beta|^2 = 1.
\end{equation}

Essa definição formal diferencia o qubit de um bit clássico. Enquanto o bit clássico assume um valor determinístico $0$ ou $1$, o qubit é descrito por amplitudes complexas, cuja interpretação física está associada às probabilidades de resultados de medições.

\subsection{Representação Geométrica: Esfera de Bloch}

A representação geométrica dos estados de um qubit é frequentemente feita por meio da Esfera de Bloch. Nessa representação, todo estado puro de um qubit corresponde a um ponto sobre a superfície de uma esfera unitária.

Qualquer estado $\ket{\psi}$ pode ser parametrizado como
\begin{equation}
\ket{\psi} = \cos\left(\frac{\theta}{2}\right)\ket{0} + e^{i\phi}\sin\left(\frac{\theta}{2}\right)\ket{1},
\end{equation}
com $\theta \in [0,\pi]$ e $\phi \in [0,2\pi)$.

Os polos da esfera correspondem aos estados da base computacional, com $\ket{0}$ no polo norte e $\ket{1}$ no polo sul. Estados em superposição correspondem a pontos intermediários, enquanto a fase relativa $\phi$ representa rotações em torno do eixo $z$.

A Esfera de Bloch fornece uma visualização intuitiva para operações unitárias de um único qubit, que se manifestam como rotações da esfera.

\subsection{Estados Puros e Estados Mistos}

Estados quânticos podem ser classificados em estados puros e estados mistos. Estados puros são aqueles que podem ser descritos por um único vetor de estado $\ket{\psi}$, conforme apresentado anteriormente.

Estados mistos surgem quando há incerteza clássica sobre qual estado puro descreve o sistema ou quando o sistema está correlacionado com um ambiente externo. Esses estados são descritos formalmente pelo operador densidade $\rho$, definido como
\begin{equation}
\rho = \sum_i p_i \ket{\psi_i}\bra{\psi_i},
\end{equation}
onde $\{p_i\}$ é uma distribuição de probabilidades clássica.

Estados puros correspondem a operadores densidade do tipo $\rho = \ket{\psi}\bra{\psi}$, enquanto estados mistos satisfazem $\mathrm{Tr}(\rho^2) < 1$. A formulação por operadores densidade é essencial para a descrição de sistemas abertos, ruído e canais quânticos.

\subsection{Medidas Projetivas}

A medição em mecânica quântica é descrita formalmente por operadores de medição. No caso mais simples, consideram-se medidas projetivas, associadas a um conjunto de projetores ortogonais $\{P_i\}$ que satisfazem
\begin{equation}
P_i P_j = \delta_{ij} P_i, \quad \sum_i P_i = I.
\end{equation}

A probabilidade de obter o resultado associado ao projetor $P_i$, quando o sistema está no estado $\rho$, é dada pela regra de Born:
\begin{equation}
p(i) = \mathrm{Tr}(P_i \rho).
\end{equation}

Após a medição, o estado do sistema colapsa para
\begin{equation}
\rho' = \frac{P_i \rho P_i}{\mathrm{Tr}(P_i \rho)}.
\end{equation}

No contexto da computação quântica, a medição mais comum é realizada na base computacional, correspondendo aos projetores $\ket{0}\bra{0}$ e $\ket{1}\bra{1}$.

\subsection{Comparação entre Bit Clássico e Qubit}

A diferença entre um bit clássico e um qubit não se limita à possibilidade de superposição. Um bit clássico pode ser descrito completamente por uma variável binária ou, em contextos probabilísticos, por uma distribuição de probabilidades clássica.

O qubit, por sua vez, é descrito por amplitudes complexas, o que permite fenômenos como interferência quântica. Além disso, estados quânticos não podem ser copiados arbitrariamente, conforme estabelecido pelo teorema da não clonagem, e a medição altera inevitavelmente o estado do sistema.

Essas diferenças impõem restrições e, simultaneamente, oferecem novos recursos computacionais que não possuem análogos clássicos.

\subsection{Introdução à Informação Quântica}

A informação quântica estuda como a informação é representada, processada e transmitida quando codificada em sistemas quânticos. Diferentemente da informação clássica, ela está sujeita às leis da mecânica quântica, incluindo superposição, emaranhamento e colapso por medição.

\subsubsection{Entropia Quântica}

A entropia quântica, conhecida como entropia de von Neumann, generaliza a entropia de Shannon para estados quânticos. Ela é definida como
\begin{equation}
S(\rho) = -\mathrm{Tr}(\rho \log \rho).
\end{equation}

Para estados puros, a entropia é nula, refletindo a ausência de incerteza. Para estados mistos, a entropia é positiva e quantifica o grau de mistura ou incerteza do estado.

\subsubsection{Canal Quântico}

Um canal quântico descreve a evolução mais geral possível de um estado quântico, incluindo efeitos de ruído e interação com o ambiente. Formalmente, um canal quântico é uma aplicação linear completamente positiva e preservadora de traço, que mapeia operadores densidade em operadores densidade.

Canais quânticos são fundamentais para o estudo de comunicação quântica, correção de erros e limitações físicas da computação quântica em sistemas reais.




